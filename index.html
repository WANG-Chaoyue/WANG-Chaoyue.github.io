<!DOCTYPE html>
<html>
<title>Chaoyue Wang's Homepage</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="description" content="Chaoyue Wang is a research scientist at JD Explore Academy (JD.com).">
<meta name="keywords" content="Chaoyue Wang, 王超岳, wangchoayue, Chaoyue, Wang, Deep Learning, TJU, USYD, Computer, Vision, GAN, Transformer, homepage">
<meta name="author" content="Chaoyue Wang" />

<link rel="stylesheet" href="w3.css">
<link rel="stylesheet" href="font-awesome.min.css">
<link rel="stylesheet" href="stylesheet.css">


<style>
.w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
	
body, html {
  height: 100%;
  line-height: 1.8;
}
.w3-bar .w3-button {
  padding: 16px;
}
</style>

<style>
			.a1{
				font-weight:bold;
			}
			.a2{
				font-weight:bolder;
			}
</style>

<link rel="icon" type="image/jpg" href="images/icon.jpg">
<body>

<!-- Navbar (sit on top) -->
<div class="w3-top">
  <div class="w3-bar w3-white w3-card" id="myNavbar">
    <a class="w3-bar-item w3-button w3-wide">CHAOYUE</a>
    <!-- Right-sided navbar links -->
    <div class="w3-right w3-hide-small">
      <a href="#home" class="w3-bar-item w3-button">Home</a>
      <a href="#news" class="w3-bar-item w3-button">News</a>
      <a href="#publications" class="w3-bar-item w3-button">Reserach</a>
      <a href="#talks" class="w3-bar-item w3-button">Talks</a>
      <a href="#service" class="w3-bar-item w3-button">Service</a>
      <a href="#awards" class="w3-bar-item w3-button">Awards</a>
    </div>
    <!-- Hide right-floated links on small screens and replace them with a menu icon -->

    <a href="javascript:void(0)" class="w3-bar-item w3-button w3-right w3-hide-large w3-hide-medium" onclick="w3_open()">
      <i class="fa fa-bars"></i>
    </a>
  </div>
</div>

<!-- Sidebar on small screens when clicking the menu icon -->
<nav class="w3-sidebar w3-bar-block w3-black w3-card w3-animate-left w3-hide-medium w3-hide-large" style="display:none" id="mySidebar">
  <a href="javascript:void(0)" onclick="w3_close()" class="w3-bar-item w3-button w3-large w3-padding-16">Close ×</a>
  <a href="#home" onclick="w3_close()" class="w3-bar-item w3-button">Home</a>
  <a href="#news" onclick="w3_close()" class="w3-bar-item w3-button">News</a>
  <a href="#publications" onclick="w3_close()" class="w3-bar-item w3-button">Reserach</a>
  <a href="#talks" onclick="w3_close()" class="w3-bar-item w3-button">Talks</a>
  <a href="#service" onclick="w3_close()" class="w3-bar-item w3-button">Service</a>
  <a href="#awards" onclick="w3_close()" class="w3-bar-item w3-button">Awards</a>
</nav>

<!-- Home Section -->
<div class="w3-container w3-center w3-white w3-padding-32" style="padding:128px 16px" id="home">
  <img style="width: 80%;max-width: 300px" alt="profile photo" src="images/chaoyue.jpg">
  <h2 class="w3-center">Chaoyue Wang</h2>
  
  <p class="w3-middle w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  Hi! My name is Chaoyue Wang, a researcher specializing in generative models and artificial intelligence-generated content (AIGC) at JD Explore Academy. Prior to that, I worked as a postdoctoral researcher in machine learning and generative modeling at the School of Computer Science, University of Sydney. I obtained my bachelor's degree from Tianjin University in 2014 and my PhD degree from the University of Technology Sydney in 2018.
  </p>	  
  <p class="w3-middle w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  My research primarily focuses on addressing three fundamental challenges in generative modeling: "Generative Model Training," "Controllable and Interpretable Generative Models," and "Generalized Generative Applications." I have authored over 30 academic works published in prestigious journals and conferences such as IEEE T-PAMI, T-EVC, IEEE T-IP, NeurIPS, CVPR, ECCV, IJCAI, among others. My research has garnered recognition from the academic community, including receiving the Distinguished Student Paper Award at IJCAI-17 for my first-author work on TDGAN and having my first-author work on E-GAN selected as Best of the Physics arXiv by MIT TR and Best of GAN Papers in the Year 2018 by DTRANSPOSED. Additionally, I played a leading role in writing the first official white paper on AIGC.
  </p>
  <p class="w3-middle w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  My current research interests center on developing foundation generative models that are generalizable, interpretable, and controllable. I currently focus on conducting cutting-edge research in the directions of "Pretraining of Foundation Generative Models," "Disentangled Feature Learning within Pretrained Generative Models," and "Multimodal Data Fine-tuning of Pre-trained Generative Models." If you are also interested in these topics or related applications, please feel free to contact me at chaoyue[dot]wang[at]outlook[dot]com.
  </p>
  <p class="w3-center w3-middle">
          <a href="mailto:chaoyue.wang@sydney.edu.au">Email</a> &nbsp/ &nbsp
          <a href="https://scholar.google.com.au/citations?user=ioj1BycAAAAJ&hl=en">Google Scholar</a>
        </p>
  </tbody></table>
</div>

<!-- The News Section -->
<div class="w3-container w3-light-grey w3-padding-32" id="news">
  <p style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  <h2>News</h2>
  </p>
  <ul>
  <li> 12/2022, Invited Talk, <a href="https://ict.caict.ac.cn/app/web/parallel.php?showtype=4">“Discovering the Splendor of AIGC: Generative Models with Explainable, Controllable, and Extendable Capabilities”</a>, <a href="https://www.iyiou.com/news/202212301040225">“ICT+ In-depth Observation and Reporting - East China Forum”</a>, CAICT.</li>
  <li> 09/2022, Invited Talk, <a href="https://yocsef.ccf.org.cn/YOCSEF/Branches/Hangzhou/News/club/2022-09-28/774462.shtml">“Evolution and Prospect of Functional Model and Super Deep Learning”</a>, Hold by CCF YOCSEF.</li>
  <li> 09/2022, Invited Talk, <a href="https://mp.weixin.qq.com/s/aPtDhY7PDJ4oQ5bqW4e3ag">“Introduction on Artificial Intelligence Generated Content (AIGC)”</a>, Hold by 2022 WAIC (World Artificial Intelligence Conference).</li>
  <li> 09/2022, We and the China Academy of Information and Communications Technology (CAICT) jointly published the <a href="http://www.caict.ac.cn/kxyj/qwfb/bps/202209/t20220902_408420.htm">White Paper on Artificial Intelligence Generated Content (AIGC)</a>.</li>
  <li> 09/2022, Our project "Super Deep Learning of JD Explore Academy" has been selected as the <a href="https://business.cctv.com/2022/08/12/ARTIikGbPq3fjZQBcR2fygwj220812.shtml"> TOP30 rank of SAIL (Superior AI Leader) Award</a> at 2022 WAIC (World Artificial Intelligence Conference).</li>
  <li> 05/2022, Invited Talk, <a href="https://www.bilibili.com/video/BV1B3411N7Tx/?from=search&vd_source=f07744c9654f75a87c3d0c64b6acfb5e">“AIGC: Multimodal Content Generation using AI Algorithms”</a>, Hold by China Graphics Society (CGS).</li>  
  <li> 06/2020, Invited Talk, <a href="https://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649802944&idx=1&sn=815fc7a9af245750664cd844bec98848&chksm=beca271889bdae0e896ebf34743286a821667c9cd7be1c445c0eabccc039dc23c2f4af23c0b5&scene=27#wechat_redirect">“Exploration and Application of Deep Representation Editing in CV Tasks”</a>, Hold by CAAI.</li>
  <li> 12/2019, Invited Talk, <a href="http://www.multimediauts.org/VCIP_2019_V1/dl_tutorials.html">“Stabilizing GANs training via evolutionary strategy”</a>, VCIP-2019.</li>
  <li> 08/2019, One invited popular science paper has been published in <a href="https://huanqiukexue.com/">Scientific American (Chinese version)</a>.</li>
  <li> 03/2018, Our paper 'EGAN' has been selected as <a href="https://www.technologyreview.com/2018/03/17/144612/the-best-of-the-physics-arxiv-week-ending-march-17-2018/">the Best Paper of the Physics arXiv</a> by MIT Technology Review.</li>
  <li> 08/2017, Our paper 'TDGAN' has been selected as Distinguished Student Paper Award, IJCAI-17.</li>
  </ul>
    
</div>


<!-- The Publications Section Main Screen-->
<div class="w3-container w3-white w3-padding-32" id="publications">
  
  <p style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  <h2>Selected Publications </h2>
  </p>
  <h5> * indicates Equal Contribution; <sup>#</sup>indicates (Co-)Supervised Students</h5>

 <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/ETR.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Exposure Trajectory Recovery from Motion Blur</strong><br>
      Youjian Zhang<sup>#</sup>, <strong>Chaoyue Wang</strong>, Stephen J. Maybank, and Dacheng Tao<br>
      <em>IEEE TPAMI</em> 2021 [<a style="color: #447ec9" href="https://drive.google.com/file/d/1GJpwr6kXsqx___VpbHrPCoK78gQrfXQV/view?usp=sharing">PDF</a>] [<a style="color: #447ec9" href="https://github.com/yjzhang96/Motion-ETR">Code</a>] [<a style="color: #447ec9" href="https://drive.google.com/file/d/1UY9a6DrMyy6RUmmjHMH-8ihSzNBLSibi/view?usp=sharing">Video</a>]
     <br /> <br />	
  </div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/ETR.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Exposure Trajectory Recovery from Motion Blur</strong><br>
          Youjian Zhang<sup>#</sup>, <strong>Chaoyue Wang</strong>, Stephen J. Maybank, and Dacheng Tao<br>
      <em>IEEE TPAMI</em> 2021 [<a style="color: #447ec9" href="https://drive.google.com/file/d/1GJpwr6kXsqx___VpbHrPCoK78gQrfXQV/view?usp=sharing">PDF</a>] [<a style="color: #447ec9" href="https://github.com/yjzhang96/Motion-ETR">Code</a>] [<a style="color: #447ec9" href="https://drive.google.com/file/d/1UY9a6DrMyy6RUmmjHMH-8ihSzNBLSibi/view?usp=sharing">Video</a>]
    </table>
  </div>
  <p> </p>
	  
  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/MRIDistill.jpg" class="papericon" width=300>
      <div class="media-body"><strong>MRI-based Alzheimer's Disease Prediction via Distilling the Knowledge in Multi-modal Data</strong><br>
      Hao Guan<sup>#</sup>, <strong>Chaoyue Wang</strong>, and Dacheng Tao<br>
      <em>NeuroImage</em> 2021 [<a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S1053811921008594?dgcid=rss_sd_all">PDF</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/MRIDistill.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>MRI-based Alzheimer's Disease Prediction via Distilling the Knowledge in Multi-modal Data</strong><br>
      Hao Guan<sup>#</sup>, <strong>Chaoyue Wang</strong>, and Dacheng Tao<br>
      <em>NeuroImage</em> 2021 [<a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S1053811921008594?dgcid=rss_sd_all">PDF</a>]
    </table>
  </div>
  <p> </p> 
	  
  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/QGAN.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Experimental Quantum Generative Adversarial Networks for Image Generation</strong><br>
      He-Liang Huang*, Yuxuan Du*, Ming Gong, Youwei Zhao, Yulin Wu, <strong>Chaoyue Wang</strong>, Shaowei Li, Futian Liang, Jin Lin, Yu Xu, Rui Yang, Tongliang Liu, Min-Hsiu Hsieh, Hui Deng, Hao Rong, Cheng-Zhi Peng, Chaoyang Lu, Yu-Ao Chen, Dacheng Tao, Xiaobo Zhu, and Jian-Wei Pan<br>
      <em>Physical Review Applied</em> 2021 [<a style="color: #447ec9" href="https://arxiv.org/pdf/2010.06201.pdf">PDF</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/QGAN.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Experimental Quantum Generative Adversarial Networks for Image Generation</strong><br>
      He-Liang Huang*, Yuxuan Du*, Ming Gong, Youwei Zhao, Yulin Wu, <strong>Chaoyue Wang</strong>, Shaowei Li, Futian Liang, Jin Lin, Yu Xu, Rui Yang, Tongliang Liu, Min-Hsiu Hsieh, Hui Deng, Hao Rong, Cheng-Zhi Peng, Chaoyang Lu, Yu-Ao Chen, Dacheng Tao, Xiaobo Zhu, and Jian-Wei Pan<br>
      <em>Physical Review Applied</em> 2021 [<a style="color: #447ec9" href="https://arxiv.org/pdf/2010.06201.pdf">PDF</a>]
    </table>
  </div>
  <p> </p> 

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/UTI-VFI.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Video Frame Interpolation without Temporal Priors</strong><br>
      Youjian Zhang<sup>#</sup>*, <strong>Chaoyue Wang*</strong>, and Dacheng Tao<br>
      <em>NeurIPS</em> 2020 [<a style="color: #447ec9" href="https://proceedings.neurips.cc/paper/2020/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/yjzhang96/UTI-VFI">Code</a>] [<a style="color: #447ec9" href="https://drive.google.com/file/d/1eRUzA2m3EvvrHw0MhO4AMxEQEgdt8LGc/view">Video</a>]
      <br /> <br />
  </div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/UTI-VFI.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Video Frame Interpolation without Temporal Priors</strong><br>
          Youjian Zhang*<sup>#</sup>, <strong>Chaoyue Wang*</strong>, and Dacheng Tao<br>
      <em>NeurIPS</em> 2020 [<a style="color: #447ec9" href="https://proceedings.neurips.cc/paper/2020/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/yjzhang96/UTI-VFI">Code</a>] [<a style="color: #447ec9" href="https://drive.google.com/file/d/1eRUzA2m3EvvrHw0MhO4AMxEQEgdt8LGc/view">Video</a>]
    </table>
  </div>
  <p> </p>

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/FeFlow.jpg" class="papericon" width=300>
      <div class="media-body"><strong>FeatureFlow: Robust Video Interpolation via Structure-to-texture Generation</strong><br>
      Shurui Gui<sup>#</sup>*, <strong>Chaoyue Wang*</strong>, Qihua Chen<sup>#</sup>, and Dacheng Tao<br>
      <em>CVPR</em> 2020 [<a style="color: #447ec9" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gui_FeatureFlow_Robust_Video_Interpolation_via_Structure-to-Texture_Generation_CVPR_2020_paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/CM-BF/FeatureFlow">Code</a>] [<a style="color: #447ec9" href="https://github.com/CM-BF/storage/tree/master/videos">Video</a>]
     <br /> <br />
  </div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/FeFlow.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>FeatureFlow: Robust Video Interpolation via Structure-to-texture Generation</strong><br>
      Shurui Gui<sup>#</sup>*, <strong>Chaoyue Wang*</strong>, Qihua Chen<sup>#</sup>, and Dacheng Tao<br>
      <em>CVPR</em> 2020 [<a style="color: #447ec9" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gui_FeatureFlow_Robust_Video_Interpolation_via_Structure-to-Texture_Generation_CVPR_2020_paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/CM-BF/FeatureFlow">Code</a>] [<a style="color: #447ec9" href="https://github.com/CM-BF/storage/tree/master/videos">Video</a>]
    </table>
  </div>
  <p> </p>  

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/SAAT.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Syntax-Aware Action Targeting for Video Captioning</strong><br>
      Qi Zheng<sup>#</sup>, <strong>Chaoyue Wang</strong>, and Dacheng Tao<br>
      <em>CVPR</em> 2020 [<a style="color: #447ec9" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Syntax-Aware_Action_Targeting_for_Video_Captioning_CVPR_2020_paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/SydCaption/SAAT">Code</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/SAAT.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Syntax-Aware Action Targeting for Video Captioning</strong><br>
      Qi Zheng<sup>#</sup>, <strong>Chaoyue Wang</strong>, and Dacheng Tao<br>
      <em>CVPR</em> 2020 [<a style="color: #447ec9" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Syntax-Aware_Action_Targeting_for_Video_Captioning_CVPR_2020_paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/SydCaption/SAAT">Code</a>]
    </table>
  </div>
  <p> </p>

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/PuppeteerGAN.jpg" class="papericon" width=300>
      <div class="media-body"><strong>PuppeteerGAN: Arbitrary Portrait Animation with Semantic-aware Appearance Transformation</strong><br>
      Zhuo Chen<sup>#</sup>, <strong>Chaoyue Wang</strong>, Bo Yuan, and Dacheng Tao<br>
      <em>CVPR</em> 2020 [<a style="color: #447ec9" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_PuppeteerGAN_Arbitrary_Portrait_Animation_With_Semantic-Aware_Appearance_Transformation_CVPR_2020_paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://www.youtube.com/watch?v=qcRlxI4Q-iI">Video</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/PuppeteerGAN.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>PuppeteerGAN: Arbitrary Portrait Animation with Semantic-aware Appearance Transformation</strong><br>
      Zhuo Chen<sup>#</sup>, <strong>Chaoyue Wang</strong>, Bo Yuan, and Dacheng Tao<br>
      <em>CVPR</em> 2020 [<a style="color: #447ec9" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_PuppeteerGAN_Arbitrary_Portrait_Animation_With_Semantic-Aware_Appearance_Transformation_CVPR_2020_paper.pdf">PDF</a>] [<a style="color: #447ec9" href="https://www.youtube.com/watch?v=qcRlxI4Q-iI">Video</a>]
    </table>
  </div>
  <p> </p>
  
  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/motion-adaption.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Self-supervised Pose Adaptation for Cross-Domain Image Animation</strong><br>
      <strong>Chaoyue Wang</strong>, Chang Xu, and Dacheng Tao<br>
      <em>IEEE TAI</em> 2020 [<a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9229197?denied=">Paper</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/motion-adaption.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Self-supervised Pose Adaptation for Cross-Domain Image Animation</strong><br>
      <strong>Chaoyue Wang</strong>, Chang Xu, and Dacheng Tao<br>
      <em>IEEE TAI</em> 2020 [<a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9229197?denied=">Paper</a>]
    </table>
  </div>
  <p> </p>       

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/MSGAN.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Multistage GAN for Fabric Defect Detection</strong><br>
      Juhua Liu*, <strong>Chaoyue Wang*</strong>, Hai Su, Bo Du, and Dacheng Tao<br>
      <em>IEEE TIP</em> 2020 [<a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/8937049">Paper</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/MSGAN.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Multistage GAN for Fabric Defect Detection</strong><br>
      Juhua Liu*, <strong>Chaoyue Wang*</strong>, Hai Su, Bo Du, and Dacheng Tao<br>
      <em>IEEE TIP</em> 2020 [<a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/8937049">Paper</a>]
    </table>
  </div>
  <p> </p> 

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/EGAN.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Evolutionary Generative Adversarial Networks</strong><br>
      <strong>Chaoyue Wang</strong>, Chang Xu, Xin Yao, and Dacheng Tao<br>
      <em>IEEE TEVC</em> 2019 [<a style="color: #447ec9" href="https://arxiv.org/pdf/1803.00657.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/WANG-Chaoyue/EvolutionaryGAN-pytorch">Code</a>] [<a style="color: #447ec9" href="https://www.youtube.com/watch?v=ni6P5KU3SDU">Video</a>] (<strong>The Best Paper of the Physics arXiv</strong>, selected by MIT Technology Review)
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/EGAN.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Evolutionary Generative Adversarial Networks</strong><br>
      <strong>Chaoyue Wang</strong>, Chang Xu, Xin Yao, and Dacheng Tao<br>
      <em>IEEE TEVC</em> 2019 [<a style="color: #447ec9" href="https://arxiv.org/pdf/1803.00657.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/WANG-Chaoyue/EvolutionaryGAN-pytorch">Code</a>] [<a style="color: #447ec9" href="https://www.youtube.com/watch?v=ni6P5KU3SDU">Video</a>] (<strong>The Best Paper of the Physics arXiv</strong>, selected by MIT Technology Review)
    </table>
  </div>
  <p> </p>  

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/PAN.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Perceptual Adversarial Networks for Image-to-Image Transformation</strong><br>
      <strong>Chaoyue Wang</strong>, Chang Xu, Chaohui Wang, and Dacheng Tao<br>
      <em>IEEE TIP</em> 2018 [<a style="color: #447ec9" href="https://arxiv.org/pdf/1706.09138.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/WANG-Chaoyue/PAN">Code</a>] [<a style="color: #447ec9" href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652000178&idx=3&sn=5a769009e84cbcc1070a232411314dac&chksm=f1213b43c656b25570ee7d9c6e905c231989517aea9e3cfb89b1060d6574ec52abdfb8dc6c2e&mpshare=1&scene=1&srcid=0702lJz26NItvOgRxUZLeTKj#rd">Related report</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/PAN.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Perceptual Adversarial Networks for Image-to-Image Transformation</strong><br>
      <strong>Chaoyue Wang</strong>, Chang Xu, Chaohui Wang, and Dacheng Tao<br>
      <em>IEEE TIP</em> 2018 [<a style="color: #447ec9" href="https://arxiv.org/pdf/1706.09138.pdf">PDF</a>] [<a style="color: #447ec9" href="https://github.com/WANG-Chaoyue/PAN">Code</a>] [<a style="color: #447ec9" href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652000178&idx=3&sn=5a769009e84cbcc1070a232411314dac&chksm=f1213b43c656b25570ee7d9c6e905c231989517aea9e3cfb89b1060d6574ec52abdfb8dc6c2e&mpshare=1&scene=1&srcid=0702lJz26NItvOgRxUZLeTKj#rd">Related report</a>]
    </table>
  </div>
  <p> </p>

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/TDGAN.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Tag Disentangled Generative Adversarial Network for Object Image Re-rendering</strong><br>
      <strong>Chaoyue Wang</strong>, Chaohui Wang, Chang Xu, and Dacheng Tao<br>
      <em>IJCAI</em> 2017 [<a style="color: #447ec9" href="https://www.ijcai.org/proceedings/2017/0404.pdf">PDF</a>] [<a style="color: #447ec9" href="https://zhuanlan.zhihu.com/p/66974261">Related report</a>] (<strong>Distinguished Student Paper Award</strong>, 1 out of 2540)
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/TDGAN.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Tag Disentangled Generative Adversarial Network for Object Image Re-rendering</strong><br>
      <strong>Chaoyue Wang</strong>, Chaohui Wang, Chang Xu, and Dacheng Tao<br>
      <em>IJCAI</em> 2017 [<a style="color: #447ec9" href="https://www.ijcai.org/proceedings/2017/0404.pdf">PDF</a>] [<a style="color: #447ec9" href="https://zhuanlan.zhihu.com/p/66974261">Related report</a>] (<strong>Distinguished Student Paper Award</strong>, 1 out of 2540)
    </table>
  </div>
  <p> </p>  

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/MSLF.jpg" class="papericon" width=300>
      <div class="media-body"><strong>Multiple Sclerosis Lesion Filling using a Non-Lesion Attention based Convolutional Network</strong><br>
      Hao Xiong*, <strong>Chaoyue Wang*</strong>, Michael Barnett and Chenyu Wang<br>
      <em>ICONIP</em> 2020 [<a style="color: #447ec9" href="https://link.springer.com/chapter/10.1007/978-3-030-63830-6_38">Paper</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/MSLF.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>Multiple Sclerosis Lesion Filling using a Non-Lesion Attention based Convolutional Network</strong><br>
      Hao Xiong*, <strong>Chaoyue Wang*</strong>, Michael Barnett and Chenyu Wang<br>
      <em>ICONIP</em> 2020 [<a style="color: #447ec9" href="https://link.springer.com/chapter/10.1007/978-3-030-63830-6_38">Paper</a>]
    </table>
  </div>
  <p> </p> 

  <!-- One paper insert -->    
  <div class="w3-hide-large">
      <img src="images/DamIde.jpg" class="papericon" width=300>
      <div class="media-body"><strong>A novel deep learning-based method for damage identification of smart building structures</strong><br>
      Yang Yu, <strong>Chaoyue Wang</strong>, Xiaoyu Gu, and Jianchun Li<br>
      <em>Structural Health Monitoring</em> 2019 [<a style="color: #447ec9" href="https://journals.sagepub.com/doi/full/10.1177/1475921718804132">Paper</a>]
     <br /> <br />
	</div></div>
  <div class="w3-hide-small w3-hide-medium">
    <table class="imgtable"><tr valign="top">
    <tr>
    <td><img src="images/DamIde.jpg" class="papericon" width=220></td>
    <td align="left">
      <strong>A novel deep learning-based method for damage identification of smart building structures</strong><br>
      Yang Yu, <strong>Chaoyue Wang</strong>, Xiaoyu Gu, and Jianchun Li<br>
      <em>Structural Health Monitoring</em> 2019 [<a style="color: #447ec9" href="https://journals.sagepub.com/doi/full/10.1177/1475921718804132">Paper</a>]
    </table>
  </div>
  <p> </p> 

</div>

<!-- The Talks Section -->
<div class="w3-container w3-light-grey w3-padding-32" id="talks">
  <p style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  <h2>Talks</h2>
  </p>
  <ul>
  <li> 06/2020, Invited Talk, <a href="https://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649802944&idx=1&sn=815fc7a9af245750664cd844bec98848&chksm=beca271889bdae0e896ebf34743286a821667c9cd7be1c445c0eabccc039dc23c2f4af23c0b5&scene=27#wechat_redirect">“Exploration and Application of Deep Representation Editing in CV Tasks”</a>, Hold by CAAI [<a style="color: #447ec9" href="https://github.com/WANG-Chaoyue/WANG-Chaoyue.github.io/blob/main/slides/CAAI-ChaoyueWang.pdf">Slides</a>]</li>
  <li> 12/2019, Invited Talk, <a href="http://www.multimediauts.org/VCIP_2019_V1/dl_tutorials.html">“Stabilizing GANs training via evolutionary strategy”</a>, VCIP-2019 [<a style="color: #447ec9" href="https://github.com/WANG-Chaoyue/WANG-Chaoyue.github.io/blob/main/slides/VCIP2019-ChaoyueWANG.pdf">Slides</a>]</li>
  </ul>
    
</div>

<!-- The Service Section -->
<div class="w3-container w3-white w3-padding-32" id="service">
  <p style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  <h2>Service</h2>
  </p>  
  
  <h5> <strong>Conference Senior Program Committee (SPC) Member</strong> </h5>
  <p style="margin-bottom:-16px">
  <li> International Joint Conferences on Artificial Intelligence (IJCAI)</li>
  <li>AAAI Conference on Artificial Intelligence (AAAI)</li>
  </p>  

  <h5> <strong>Conference Program Committee (PC) Member</strong> </h5>
  <p style="margin-bottom:-16px">
  <li>Neural Information Processing Systems (NeurIPS)</li>
  <li>International Conference on Machine Learning (ICML)</li>
  <li>International Conference on Learning Representations (ICLR)</li>
  <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
  <li>International Conference on Computer Vision (ICCV)</li>
  <li>European Conference on Computer Vision (ECCV)</li>
  <li>ACM Multimedia (ACM MM)</li>
  <li>IEEE International Conference on Data Mining (ICDM)</li>
  <li>International Conference on Database Systems for Advanced Applications (DASFAA)</li>
  </p>  

  <h5> <strong>Journal Reviewer</strong> </h5>
  <p style="margin-bottom:-16px">
  <li>International Journal of Computer Vision (IJCV)</li>
  <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE T-PAMI)</li>
  <li>IEEE Transactions on Neural Networks and Learning Systems (IEEE T-NNLS)</li>
  <li>IEEE Transactions on Evolutionary Computation (IEEE T-EVC)</li>
  <li>IEEE Transactions on Image Processing (IEEE T-IP)</li>
  <li>IEEE Transactions on Cybernetics (IEEE T-CYB)</li>
  <li>IEEE Transactions on Circuits and Systems for Video Technology (IEEE T-CSVT)</li>
  </p>  
</div>

<!-- The Awards Section -->
<div class="w3-container w3-light-grey w3-padding-32" id="awards">
  <p style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:1200px">
  <h2>Selected Awards</h2>
  </p>
  <h6> [2018] <strong>The Best Paper of the Physics arXiv</strong> (selected by MIT Technology Review) </h6>
  <p style="margin-bottom:-12px">
  <li> Week’s most thought-provoking papers from the Physics arXiv (week ending Mar. 17, 2018)</li>
  </p>

  <h6> [2017] <strong>Distinguished Student Paper Award</strong> (selected by IJCAI-17) </h6>
  <p style="margin-bottom:-12px">
  <li> The 26th International Joint of Conference on Artificial Intelligence (1 selected out of 2540 submissions)</li>
  </p>

  <h6> [2012] <strong>Tianjin Physics Contest in common colleges and universities</strong>, Tianjin, China </h6>
  <p style="margin-bottom:-12px">
  <li> Grand prize (15 selected out of about 1500 undergraduates) </li>
  </p>

  <h6> [2012] <strong>Scholarship of Excellent Student Cadre</strong>, Tianjin University </h6>
  <p style="margin-bottom:-12px">
  <li> Awarded for outstanding performance in Tianjin University every year</li>
  </p>

  <h6> [2019] <strong>Chinese Physics Olympiad </strong>, China </h6>
  <p style="margin-bottom:-12px">
  <li> Second prize (35 selected out of all high school students in Yunnan province) </li>
  </p>

  <h6> [2009] <strong>The national senior high school students applied physics knowledge competition </strong>, Yunnan, China </h6>
  <p style="margin-bottom:-12px">
  <li> Placed first in Yunnan province</li>      
  </p>

</div>


<!-- Footer -->
<footer class="w3-center w3-black w3-padding-64">
  <a href="#home" class="w3-button w3-light-grey"><i class="fa fa-arrow-up w3-margin-right"></i>To the top</a>
  <p></p>
  
  <p>Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-text-green">w3.css</a></p>
</footer>
 

<script>



// Toggle between showing and hiding the sidebar when clicking the menu icon
var mySidebar = document.getElementById("mySidebar");

function w3_open() {
  if (mySidebar.style.display === 'block') {
    mySidebar.style.display = 'none';
  } else {
    mySidebar.style.display = 'block';
  }
}

// Close the sidebar with the close button
function w3_close() {
    mySidebar.style.display = "none";
}
</script>

</body>
</html>
